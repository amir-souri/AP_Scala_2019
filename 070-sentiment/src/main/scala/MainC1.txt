/*1. What data sets have you managed to run (including size)?
2. What accuracy have you obtained? With how many iterations? And with what layer configuration?
Show the variance of accuracy observed in cross validation.

3. What degree of parallelization was obtained? (for instance contrast the wall clock time with
the CPU time). Note we are not asking you to optimize parallelization, just to report what you
obtained.
4. What extensions (if any) you have implemented? Have you tried another classifier? Another
network configuration? Running on a highly parallel cluster of machines, etc. ...*/

import org.apache.spark.ml.feature.Tokenizer
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql.{DataFrame, Row}
import org.apache.spark.rdd._
import scala.collection.mutable.WrappedArray
import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}


object Main {

  type Embedding = (String, List[Double])
  type ParsedReview = (Integer, String, Double)

  org.apache.log4j.Logger getLogger "org" setLevel (org.apache.log4j.Level.WARN)
  org.apache.log4j.Logger getLogger "akka" setLevel (org.apache.log4j.Level.WARN)
  val spark = SparkSession.builder
    .appName("Sentiment")
    .master("local[5]")
    .getOrCreate

  spark.conf.set("spark.executor.memory", "4g")

  import spark.implicits._

  val reviewSchema = StructType(
    Array(
      StructField("reviewText", StringType, nullable = false),
      StructField("overall", DoubleType, nullable = false),
      StructField("summary", StringType, nullable = false)
    )
  )

  def loadReviews(path: String): Dataset[ParsedReview] =
    spark.read
      .schema(reviewSchema)
      .json(path)
      .rdd
      .zipWithUniqueId
      .map[(Integer, String, Double)] {
        case (row, id) =>
          (id.toInt, s"${row getString 2} ${row getString 0}", row getDouble 1)
      }
      .toDS
      .withColumnRenamed("_1", "id")
      .withColumnRenamed("_2", "text")
      .withColumnRenamed("_3", "overall")
      .as[ParsedReview]

  def loadGlove(path: String): Dataset[Embedding] =
    spark.read
      .text(path)
      .map { _ getString 0 split " " }
      .map(r => (r.head, r.tail.toList.map(_.toDouble)))
      .withColumnRenamed("_1", "word")
      .withColumnRenamed("_2", "vec")
      .as[Embedding]

  def main(args: Array[String]) = {

    val DATA_PATH =
      "/media/neutron/D/3Semester/AP/Repository/Todo/070-sentiment/data"

    val glove: Dataset[Embedding] = loadGlove(s"${DATA_PATH}/glove.6B.200d.txt")
    val reviews: Dataset[ParsedReview] = loadReviews(
      s"${DATA_PATH}/Home_and_Kitchen_5.json"
    )

    val tokenizer = new Tokenizer().setInputCol("text").setOutputCol("words")
    val tokenized: DataFrame = tokenizer.transform(reviews)


    def reviewsToEmbeddings(
        transformTokenized: DataFrame
    ): Dataset[(Integer, String, Double)] = {
      transformTokenized
        .flatMap(
          a =>
            a.getAs[WrappedArray[String]]("words")
              .map(
                word =>
                  Tuple3[Integer, String, Double](
                    a.getAs("id"),
                    word,
                    a.getAs("overall")
                  )
              )
        )
        .withColumnRenamed("_1", "id")
        .withColumnRenamed("_2", "word")
        .withColumnRenamed("_3", "overall")
        .as[(Integer, String, Double)]
    }

    val singleWords = reviewsToEmbeddings(tokenized)
    
    val joinedGlove: Dataset[(String, Integer, Double, Array[Double])] =
      singleWords
        .join(glove, "word")
        .withColumnRenamed("_1", "word")
        .withColumnRenamed("_2", "id")
        .withColumnRenamed("_3", "overall")
        .withColumnRenamed("_4", "vec")
        .as[(String, Integer, Double, Array[Double])]

    val dropColumnWord: DataFrame =
      joinedGlove.toDF
        .map(
          row =>
            Tuple3[Integer, Double, WrappedArray[Double]](
              row.getAs("id"),
              row.getAs("overall"),
              row.getAs("vec")
            )
        )
        .withColumnRenamed("_1", "id")
        .withColumnRenamed("_2", "overall")
        .withColumnRenamed("_3", "vec")

    val addColumn1s: DataFrame = dropColumnWord
      .map(
        row =>
          Tuple4[Integer, Double, WrappedArray[Double], Integer](
            row.getAs("id"),
            row.getAs("overall"),
            row.getAs("vec"),
            1
          )
      )
      .withColumnRenamed("_1", "id")
      .withColumnRenamed("_2", "overall")
      .withColumnRenamed("_3", "vec")
      .withColumnRenamed("_4", "1s")

    def sumTheVectors(
        first: WrappedArray[Double],
        secound: WrappedArray[Double]
    ): WrappedArray[Double] = {
      first.zip(secound).map { case (x, y) => x + y }
    }

    def eachRowDivideVectorByCount(
        vec: WrappedArray[Double],
        count: Integer
    ): WrappedArray[Double] = {
      vec.map(_ / count)
    }

    val reduceWithKey
        : Dataset[(Integer, Double, Array[Double])] = addColumn1s.rdd
      .map(
        row =>
          (
            row.getAs("id"): Integer,
            (
              row.getAs("1s"): Integer,
              row.getAs[WrappedArray[Double]]("vec"),
              row.getAs[Double]("overall")
            )
          )
      )
      .reduceByKey((x, y) => (x._1 + y._1, sumTheVectors(x._2, y._2), x._3))
      .map(
        row =>
          (row._1, row._2._3, eachRowDivideVectorByCount(row._2._2, row._2._1))
      )
      .toDS
      .withColumnRenamed("_1", "id")
      .withColumnRenamed("_2", "overall")
      .withColumnRenamed("_3", "vec")
      .as[(Integer, Double, Array[Double])]

    val properDtaForMLPC: Dataset[(Integer, Double, Vector)] =
      reduceWithKey
        .map(
          row =>
            (
              row._1,
              if (row._2 < 3) 0 else if (row._2 == 5) 2 else 1,
              Vectors.dense(row._3)
            )
        )
        .withColumnRenamed("_1", "id")
        .withColumnRenamed("_2", "label")
        .withColumnRenamed("_3", "features")
        .as[(Integer, Double, Vector)]

    val splits = properDtaForMLPC.randomSplit(Array(0.6, 0.4), seed = 1234L)
    val train = splits(0)
    val test = splits(1)

    val layers = Array[Int](200, 4, 4, 3)
    val trainer = new MultilayerPerceptronClassifier()
      .setMaxIter(100)
      .setLayers(layers)
      .setBlockSize(128)
      .setSeed(1234L)

    val model = trainer.fit(train)

    val result = model.transform(test)
    val predictionAndLabels = result.select("prediction", "label")
    val evaluator = new MulticlassClassificationEvaluator()
      .setMetricName("accuracy")

    println(s"Test set accuracy = ${evaluator.evaluate(predictionAndLabels)}")

///////////////////////////////////////////////////


val paramGrid = new ParamGridBuilder().build()
val evaluatorCv = new MulticlassClassificationEvaluator().setMetricName("accuracy")
val cv = new CrossValidator().setEstimator(trainer) // our MultiLayerPerceptronClassifier //evaluatorAccuracy
.setEvaluator(evaluatorCv)
.setEstimatorParamMaps(paramGrid).setParallelism(4).setNumFolds(10);
val modelCV = cv.fit(train);
//val result = model.transform(test) // query the result objects for accuracy













    spark.stop
  }
}