
//filip
// Advanced Programming. Andrzej Wasowski. IT University
// To execute this example, run "sbt run" or "sbt test" in the root dir of the project
// Spark needs not to be installed (sbt takes care of it)

import org.apache.spark.ml.feature.Tokenizer
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
import scala.collection.mutable.WrappedArray
import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

// This
// (https://stackoverflow.com/questions/40015416/spark-unable-to-load-native-hadoop-library-for-your-platform)
// actually does seems to work, to eliminate the missing hadoop message.
// 'WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable'
// AW not sure if the 'hadoop missing warning' matters though.

object Main {

	type Embedding = (String, List[Double])
	type ParsedReview = (Integer, String, Double)
    type PreprocessedReview = (Integer, Vector, Integer)

	org.apache.log4j.Logger getLogger "org"  setLevel (org.apache.log4j.Level.WARN)
	org.apache.log4j.Logger getLogger "akka" setLevel (org.apache.log4j.Level.WARN)
	val spark =  SparkSession.builder
		.appName ("Sentiment")
		.master  ("local[5]")
		.getOrCreate

	spark.conf.set("spark.executor.memory", "4g")

    import spark.implicits._

	val reviewSchema = StructType(Array(
			StructField ("reviewText", StringType, nullable=false),
			StructField ("overall",    DoubleType, nullable=false),
			StructField ("summary",    StringType, nullable=false)))

	// Read file and merge the text and summary into a single text column

	def loadReviews (path: String): Dataset[ParsedReview] =
        spark
        .read
			.schema (reviewSchema)
			.json (path)
			.rdd
			.zipWithUniqueId
			.map[(Integer,String,Double)] { case (row,id) =>
          (id.toInt, s"${row getString 2} ${row getString 0}", row getDouble 1) }
			.toDS
			.withColumnRenamed ("_1", "id" )
			.withColumnRenamed ("_2", "text")
			.withColumnRenamed ("_3", "overall")
			.as[ParsedReview]

  // Load the GLoVe embeddings file

  def loadGlove (path: String): Dataset[Embedding] =
		spark
			.read
			.text (path)
      .map  { _ getString 0 split " " }
      .map  (r => (r.head, r.tail.toList.map (_.toDouble))) // yuck!
			.withColumnRenamed ("_1", "word" )
			.withColumnRenamed ("_2", "vec")
			.as[Embedding]

  def main(args: Array[String]) = {

    val DATA_PATH = "/media/neutron/D/3Semester/AP/Repository/Todo/070-sentiment/data"

    val glove  = loadGlove (s"${DATA_PATH}/glove.6B.50d.txt")
    val reviews = loadReviews (s"${DATA_PATH}/Musical_Instruments_5.json")
    val tokenizer = new Tokenizer().setInputCol("text").setOutputCol("tokenized")

    val data = tokenizer.transform(
        loadReviews(s"${DATA_PATH}/Musical_Instruments_5.json")
    ).drop("text").rdd.flatMap{
        case Row(id: Integer, rating: Double, words: WrappedArray[String]) => words.map(word => (word, (id, rating)))
    }.join(glove.rdd).map{
        case (word: String, ((id: Integer, rating: Double), vec: List[Double])) => {
            (id, (rating, vec, 1))
        }
    }.reduceByKey(
        (a: (Double, List[Double], Int), b: (Double, List[Double], Int)) => {
            val (rating_a, vec_a, count_a) = a
            val (rating_b, vec_b, count_b) = b
            (rating_a, (vec_a, vec_b).zipped.map(_ + _), count_a + count_b)
        }
    ).map{
        case (id: Integer, (rating: Double, vec: List[Double], count: Int)) => {
            (id, Vectors.dense(vec.map(_ / count).toArray), if (rating < 3) 0 else if (rating == 5) 2 else 1)
        }
    }.toDS
    .withColumnRenamed("_1", "id")
    .withColumnRenamed("_2", "features")
    .withColumnRenamed("_3", "label")
    .as[PreprocessedReview]
    
    val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)
    val train = splits(0)
    val test = splits(1)


    val layers = Array[Int](50, 5, 4, 3)

    // create the trainer and set its parameters
    val trainer = new MultilayerPerceptronClassifier()
      .setLayers(layers)
      .setBlockSize(128)
      .setSeed(1234L)
      .setMaxIter(100)

    // train the model
    val model = trainer.fit(train)

    // compute accuracy on the test set
    val result = model.transform(test)
    val predictionAndLabels = result.select("prediction", "label")
    val evaluator = new MulticlassClassificationEvaluator()
      .setMetricName("accuracy")

    println(s"Test set accuracy = ${evaluator.evaluate(predictionAndLabels)}")

	spark.stop
  }
} 